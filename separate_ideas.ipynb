{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b065c2-35b4-4d4b-a5fd-6ebda42a69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchauhan/baseline/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import functional as F\n",
    "from sentence_transformers import SentenceTransformer, models, losses, util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667b4a86-3f92-4112-8b43-e88ddc555d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Connect to GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Change to GPUs being used\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c7faef-54fd-4023-a1c8-f78ed5bcc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture cell output in case session disconnects\n",
    "today = datetime.date.today()\n",
    "so = open(str(today) + \".log\", 'w', 10) # Replace with current date\n",
    "sys.stdout.echo = so\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ce395d-b311-4544-87cc-218f99446610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in '/home/jchauhan/Legal Ideas': ['07_wiki_validation-cross-validation.ipynb', 'more_words.ipynb', 'separate_ideas.ipynb', '2023-03-02.log', 'data-wiki', '.ipynb_checkpoints', '20230213.log', '2023-03-01.log', '20230212.log', 'clean_si_results_2', 'experiments_1', 'clean_si_results', 'repeatable.ipynb', 'clean_separate_ideas.ipynb', 'experiment_2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Check current directory\n",
    "cwd = os.getcwd()\n",
    "files = os.listdir(cwd)\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efc92b7-10a8-4774-998c-6e76f52e2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(fp):\n",
    "    \"\"\"\n",
    "    Returns mapping of word to frequency from vocab file\n",
    "    \n",
    "    :param fp: Filepath to vocab\n",
    "    :return: Dictionary, word (annotated with sense) -> frequency\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    with open(fp) as f: # 'Legal Ideas/data-wiki/vocab.txt'\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            vocab[line[0]] = int(line[1])\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def get_word_senses(vocab):\n",
    "    \"\"\"\n",
    "    Returns mapping of word to sense(s)\n",
    "    \n",
    "    :param vocab: Dictionary, word (annotated with sense) -> frequency\n",
    "    :return: Dictionary, word -> sense(s)\n",
    "    \"\"\"\n",
    "    terms = [k for k in vocab if k[0] == '@' and '@' in k[1:]]\n",
    "    words = defaultdict(set)\n",
    "    for term in terms:\n",
    "        freq = vocab[term]\n",
    "        word = term[1:term[1:].index('@')+1]\n",
    "        category = term[term[1:].index('@') + 3:]\n",
    "        if freq < 1000 or len(category) == 0: # Ignore words w/o annotated sense or over 1000 sentences \n",
    "            continue\n",
    "        words[word].add(category)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f3f12f-b58f-45e5-bde3-a604b41321b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_words(words):\n",
    "    \"\"\"\n",
    "    Return mapping of word to senses for all words that have two senses\n",
    "    \n",
    "    :param words: Dictionary, word -> sense(s)\n",
    "    :return: Dictionary, word -> sense(s), for every word with two senses\n",
    "    \"\"\"\n",
    "    candidates = {}\n",
    "    for word in words:\n",
    "        if len(words[word]) == 2:\n",
    "            candidates[word] = list(words[word])\n",
    "            random.shuffle(candidates[word])\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef56905e-e3c6-4da7-86c7-49fb58516fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_generator(reader):\n",
    "    \"\"\"\n",
    "    See https://stackoverflow.com/questions/75480052/can-someone-please-explain-how-this-function-works\n",
    "    \"\"\"\n",
    "    b = reader(1024 * 1024)\n",
    "    while b:\n",
    "        yield b\n",
    "        b = reader(1024 * 1024)\n",
    "\n",
    "def count_lines(fp):\n",
    "    \"\"\"\n",
    "    :param fp: Filepath\n",
    "    :return: Int, number of \\n characters in file\n",
    "    \"\"\"\n",
    "    with open(fp, 'rb') as fp:\n",
    "        c_generator = _count_generator(fp.raw.read)\n",
    "        count = sum(buffer.count(b'\\n') for buffer in c_generator)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfb142a-474d-4620-9c18-354623fa1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(vocab, index_path, corpus_path):\n",
    "    \"\"\"\n",
    "    Builds and saves (as JSON file) mapping from word to indices of sentences containing word. \n",
    "    Retrieves the index if it already exists.\n",
    "    \n",
    "    :param vocab: Dictionary, word (annotated with sense) -> frequency\n",
    "    :param index_path: Filepath the index should be saved to or retrieved from\n",
    "    :param corpus_path: Filepath to corpus\n",
    "    :return: Dictionary, word (annotated with sense) -> list of sentence_indices\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "\n",
    "    if os.path.exists(index_path): # \n",
    "        print(\"Starting index load\")\n",
    "        with open(index_path, 'r') as f:\n",
    "            index = json.load(f)\n",
    "    else:\n",
    "        corpus_size = count_lines(corpus_path)\n",
    "        \n",
    "        print(\"Building index\")\n",
    "        with open(corpus_path) as f: # \n",
    "            for i, line in tqdm(enumerate(f), total=corpus_size):\n",
    "                line = line.split()\n",
    "                for part in line:\n",
    "                    if part in vocab:\n",
    "                        index[part].append(i)     \n",
    "        \n",
    "        print(\"Starting index dump\")\n",
    "        with open(index_path, 'w') as f:\n",
    "            json.dump(index, f)\n",
    "            \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7dcc4a8-4926-4d6b-a67f-b2e9f299d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences_by_indices(indices, corpus_path): \n",
    "    \"\"\"\n",
    "    Returns a mapping of index to sentence at index in corpus \n",
    "    \n",
    "    :param indices: List of ints, indices of sentences to gather\n",
    "    :param corpus_path: Filepath containing sentences\n",
    "    :return: Dictionary, index -> sentence in corpus at index \n",
    "    \"\"\"\n",
    "    indices_set = set(indices)\n",
    "    indices_map = {}\n",
    "    \n",
    "    corpus_size = count_lines(corpus_path)\n",
    "    with open(corpus_path) as f:\n",
    "        for i, line in tqdm(enumerate(f), total=corpus_size):\n",
    "            if i in indices_set:\n",
    "                indices_map[i] = line.strip()\n",
    "    return indices_map\n",
    "\n",
    "def get_indexed_sentences(candidates, index, corpus_path):\n",
    "    \"\"\"\n",
    "    Returns a mapping of index to sentence at index for every sentence containing a key from candidates\n",
    "    \n",
    "    :param candidates: Dictionary, word -> sense(s)\n",
    "    :return: List of ints, indices for all sentences containing words in candidates\n",
    "    \"\"\"\n",
    "    indices = set()\n",
    "    for k in candidates:\n",
    "        for v in candidates[k]:\n",
    "            word = '@{}@-{}'.format(k, v)\n",
    "            indices = indices | set(index[word])\n",
    "\n",
    "    sentences_map = load_sentences_by_indices(indices, corpus_path)\n",
    "    return sentences_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db9b755-8ba4-4e20-9c6a-d9b92de57c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_embeddings(model, sentences):\n",
    "    \"\"\"\n",
    "    Returns embeddings for sentences\n",
    "    \n",
    "    :param model: Sentence encoder\n",
    "    :param sentences: List of strings, sentences to encode\n",
    "    :return: np.array, embeddings for sentences\n",
    "    \"\"\"\n",
    "    sentences_revealed = []\n",
    "    for sentence in sentences:\n",
    "        parts = sentence.split()\n",
    "        parts_revealed = []\n",
    "        for part in parts:\n",
    "            if part[0] == '@' and '@' in part[1:]: # Removes sense annotation\n",
    "                part = part[1:part[1:].index('@') + 1]\n",
    "            parts_revealed.append(part)\n",
    "        sentences_revealed.append(' '.join(parts_revealed))\n",
    "    return model.encode(sentences_revealed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "294ee42b-ed3d-4e64-ac0b-c7d28e2b86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    Initializes Linear layers of m using Uniform Xavier initialization\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    \"\"\"\n",
    "    1 hidden layer, fully connected NN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, pdrop=.5):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(pdrop),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "        ).to(device)\n",
    "        \n",
    "        self.mlp.apply(init_weights)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.mlp(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e2532f-6703-4b05-99e5-e5b794e8387f",
   "metadata": {},
   "outputs": [],
   "source": [
    " class MLP2(nn.Module):\n",
    "    \"\"\"\n",
    "    2 hidden layers, fully connected NN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, pdrop=.5):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(pdrop),\n",
    "            nn.Linear(hidden_size, 2),\n",
    "        ).to(device)\n",
    "        \n",
    "        self.mlp.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.mlp(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b383cf4-da89-412f-ae7e-01279cd81d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for sentence embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32).to(device)\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fbd097-d382-4b39-b263-c1bac8c3c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    JSON Encoder. Handles ints, floats and numpy arrays\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c922a7c8-49f5-4030-8e7e-1040fded5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_separation(model, epochs, optimizer, embeddings, res_folder_path):\n",
    "    \"\"\"\n",
    "    For each word in embeddings, computes the separation between the word's two senses and saves the results in json files\n",
    "    \n",
    "    :param model: PyTorch model, gets trained to separate embeddings for each word's senses\n",
    "    :param epochs: Int, training epochs\n",
    "    :param optimizer: PyTorch optimizer\n",
    "    :param embeddings: Dictionary, word --> { sense 1: [embeddings], sense 2: [embeddings] }\n",
    "    :param res_folder_path: Filepath of folder to save results in (folder must already exist)\n",
    "    :return: None, saves output in files\n",
    "    \"\"\"\n",
    "\n",
    "    for word in list(embeddings.keys()):\n",
    "        word_embds = embeddings[word]\n",
    "        [a,b] = list(word_embds.keys())\n",
    "        total_sizes = [len(word_embds[a]), len(word_embds[b])]\n",
    "\n",
    "        x = np.concatenate((np.array(word_embds[a]), np.array(word_embds[b]))) # Make sure 0 is right axis\n",
    "        y = np.concatenate((np.zeros(len(word_embds[a])), np.ones(len(word_embds[b]))), axis=None)\n",
    "\n",
    "        dataset = SentenceDataset(x,y)\n",
    "\n",
    "        c_weights = [len(word_embds[b])/x.shape[0], len(word_embds[a])/x.shape[0]]\n",
    "        loss_fn = nn.CrossEntropyLoss(weight = torch.Tensor(c_weights).to(device))  \n",
    "\n",
    "        train_size = int(0.875 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        batch_size = 256\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        total_pos, total_neg, total, best_b_accuracy = 0, 0, 0, 0\n",
    "        best_i = -1\n",
    "\n",
    "        for i in tqdm(range(epochs), position=0, leave=True):   \n",
    "            # Training\n",
    "            for x_train, y_train in train_dataloader: \n",
    "                y_train = y_train.type(torch.LongTensor)\n",
    "                x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  \n",
    "                pred = model(x_train)   \n",
    "                train_loss = loss_fn(pred, y_train)  \n",
    "                train_loss.backward()  \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10, norm_type=2.0) # gradient clipping\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluation\n",
    "            if i%5 == 0:\n",
    "                model.eval()\n",
    "                r_tp, r_fp, r_fn, r_tn = 0, 0, 0, 0\n",
    "\n",
    "                with torch.no_grad(): \n",
    "                    for x_test, y_test in test_dataloader:\n",
    "                        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                        y_test = y_test.to(torch.float32) \n",
    "                        predicted_outputs = model(x_test) \n",
    "                        _, predicted = torch.max(predicted_outputs, 1)             \n",
    "\n",
    "                        pos_mask, neg_mask = y_test == 1, y_test == 0\n",
    "                        pos_indices, neg_indices = torch.nonzero(pos_mask), torch.nonzero(neg_mask)  \n",
    "                        pos_y, neg_y =  y_test[pos_indices], y_test[neg_indices]\n",
    "                        pos_y_pred, neg_y_pred = predicted[pos_indices], predicted[neg_indices]\n",
    "\n",
    "                        r_tp += (pos_y == pos_y_pred).sum().item()\n",
    "                        r_fn += (pos_y != pos_y_pred).sum().item()\n",
    "                        r_fp += (neg_y != neg_y_pred).sum().item()\n",
    "                        r_tn += (neg_y == neg_y_pred).sum().item()\n",
    "\n",
    "                        if i == 0:\n",
    "                            total_pos += pos_y.size(0)\n",
    "                            total_neg += neg_y.size(0)\n",
    "                            test_sizes = [total_neg, total_pos]\n",
    "\n",
    "                    tpr, tnr = r_tp / total_pos, r_tn / total_neg\n",
    "                    b_accuracy = (tpr + tnr) / 2\n",
    "                    precision = r_tp / (r_tp + r_fp)\n",
    "                    recall = r_tp / (r_tp + r_fn)\n",
    "\n",
    "                    f1 = 2 * (recall * precision) / (recall + precision) if recall != 0 or precision != 0 else 0\n",
    "\n",
    "                    # Performance is evaluated as balanced accuracy\n",
    "                    if b_accuracy > best_b_accuracy:\n",
    "                        best_f1, best_precision, best_recall, best_i = f1, precision, recall, i\n",
    "                        best_tpr, best_tnr, best_b_accuracy = tpr, tnr, b_accuracy         \n",
    "\n",
    "                model.train()\n",
    "\n",
    "        # Save best performance\n",
    "        with open(res_folder_path + '/{}.json'.format(word), 'w') as f:\n",
    "            json.dump([best_f1, best_precision, best_recall, best_b_accuracy, best_tnr, best_tpr, total_sizes, test_sizes, best_i], f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b5a6c8-0591-4305-96a3-17a78922476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['united_states', 'france', 'american', 'germany', 'canada', 'england', 'brazil', 'italy', 'new_zealand', 'mexico', 'south_africa', 'german', 'scotland', 'poland', 'netherlands', 'sweden', 'ireland', 'italian', 'english', 'switzerland', 'new_south_wales', 'denmark', 'melbourne', 'wales', 'portugal', 'dutch', 'swedish', 'greek', 'northern_ireland', 'milan', 'liverpool', 'scottish', 'barcelona', 'norwegian', 'kent', 'great_britain', 'uruguay', 'danish', 'essex', 'finnish', 'portuguese', 'oxford', 'hindu', 'mexican', 'surrey', 'russian', 'cambridge', 'hampshire', 'somerset', 'leeds', 'japanese', 'hungarian', 'persian', 'chinese', 'chelsea', 'derbyshire', 'richmond', 'southampton', 'celtic', 'republic_of_ireland', 'java', 'hiv', 'union', 'warwickshire', 'confederate', 'cork', 'leicestershire', 'middlesex', 'sunderland', 'nottinghamshire', 'northamptonshire', 'middlesbrough', 'serbian', 'columbia', 'dundee', 'reading', 'pc', 'czech', 'wigan', 'jewish', 'armenian', 'parma', 'darlington', 'genesis', 'glamorgan', 'albanian', 'wrexham']\n"
     ]
    }
   ],
   "source": [
    "# Get words with two senses\n",
    "vocab = get_vocab(\"data-wiki/vocab.txt\")\n",
    "words = get_word_senses(vocab)\n",
    "ts_words = get_ts_words(words)\n",
    "\n",
    "list(ts_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdaf05-492c-4257-a676-46f2123ea9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(vocab, \"data-wiki/index.json\", \"data-wiki/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46f8c3-be1e-49a5-9bfa-d83cceeda6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_map = get_indexed_sentences(ts_words, index, 'data-wiki/corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4422b7-ae24-4837-a276-4e9a0f482c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda:0') # Change to GPUs being used\n",
    "\n",
    "embeddings = { word: {\n",
    "    senses[0]: sentences_embeddings(model, [sentences_map[i] for i in index['@{}@-{}'.format(word, senses[0])]]),\n",
    "    senses[1]: sentences_embeddings(model, [sentences_map[i] for i in index['@{}@-{}'.format(word, senses[1])]]),\n",
    "} for word, senses in ts_words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2591d3-10eb-4077-9853-7a5d9314e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=.0001)  \n",
    "\n",
    "model1 = MLP1(768, 4 * 768)\n",
    "model1.to(device)\n",
    "res1_fp = \"si_results\"\n",
    "\n",
    "find_separation(model1, epochs, optimizer, embeddings, res1_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368538da-5e7f-4b97-8c58-07ba8056130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP1(768, 4 * 768)\n",
    "model2.to(device)\n",
    "res2_fp = \"si_results2\"\n",
    "\n",
    "find_separation(model2, epochs, optimizer, embeddings, res2_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8a5ba90-bb26-44b8-b170-9f8d7fec4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(fp, words):\n",
    "    \"\"\"\n",
    "    :param fp: Filepath to results file\n",
    "    :param candidates: Dictionary with keys contains \n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for word in words:\n",
    "        with open(fp + \"/{}.json\".format(word), 'r') as f:\n",
    "            [best_f1, best_precision, best_recall, best_b_accuracy, best_tnr, best_tpr, total_sizes, test_sizes, best_i] = json.load(f)\n",
    "            res[word] = best_b_accuracy\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dacd2cb5-feef-4cec-94b9-a445e2159f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Results:\n",
      "great_britain 0.419407563069804\n",
      "england 0.42508495215352454\n",
      "ireland 0.42873802951562906\n",
      "uruguay 0.43855120562145333\n",
      "scotland 0.44131963209120395\n",
      "united_states 0.44197264193617386\n",
      "new_south_wales 0.44828621284540454\n",
      "reading 0.4534919487550176\n",
      "canada 0.45355081442736955\n",
      "nottinghamshire 0.4572876447876448\n",
      "hampshire 0.46088345045486734\n",
      "warwickshire 0.46228623028158633\n",
      "union 0.4694937497219627\n",
      "northamptonshire 0.4696754563894523\n",
      "dundee 0.4705170460397569\n",
      "southampton 0.4741333490816748\n",
      "brazil 0.47562776035082793\n",
      "france 0.4785275727925571\n",
      "northern_ireland 0.4790547953115076\n",
      "japanese 0.4793795620437956\n",
      "surrey 0.4799681750235932\n",
      "wigan 0.4805020170327207\n",
      "poland 0.482544733444014\n",
      "denmark 0.48319142888082733\n",
      "italy 0.4861067428964341\n",
      "sunderland 0.4884146341463415\n",
      "confederate 0.4888315828957239\n",
      "leeds 0.48966640347414137\n",
      "oxford 0.4909090909090909\n",
      "serbian 0.4913074712643678\n",
      "american 0.4947743148844707\n",
      "finnish 0.494937030031831\n",
      "liverpool 0.49570481941684513\n",
      "german 0.49879154552113514\n",
      "czech 0.49947499045437194\n",
      "dutch 0.49962857623160484\n",
      "mexican 0.5000522495820033\n",
      "barcelona 0.500292080103997\n",
      "danish 0.5011494252873563\n",
      "persian 0.5018641606389812\n",
      "derbyshire 0.5018875278396436\n",
      "scottish 0.5022987521059996\n",
      "norwegian 0.502408342783969\n",
      "essex 0.5031358391732144\n",
      "russian 0.5032477655373103\n",
      "hungarian 0.5035395531013227\n",
      "portuguese 0.5048344130290005\n",
      "republic_of_ireland 0.5051346801346801\n",
      "switzerland 0.5052734800610155\n",
      "swedish 0.5062429057888763\n",
      "cambridge 0.5073194249649369\n",
      "java 0.507337207957437\n",
      "mexico 0.5076806031103164\n",
      "middlesbrough 0.5079015192620727\n",
      "jewish 0.5107706310679612\n",
      "kent 0.5126306029713406\n",
      "somerset 0.5135658914728682\n",
      "germany 0.5137659459677943\n",
      "cork 0.5143379906852961\n",
      "italian 0.515578125\n",
      "greek 0.5164535966202827\n",
      "hindu 0.516602809706258\n",
      "melbourne 0.5172429078014185\n",
      "richmond 0.517381557103736\n",
      "sweden 0.5189739651724442\n",
      "chinese 0.5190462119557536\n",
      "milan 0.5190812720848057\n",
      "english 0.5202449723386993\n",
      "netherlands 0.5205179710062048\n",
      "chelsea 0.5211160108548168\n",
      "columbia 0.5220340501792115\n",
      "leicestershire 0.5222043130280124\n",
      "hiv 0.5230611328500866\n",
      "celtic 0.525\n",
      "wales 0.5266159515262028\n",
      "pc 0.5270475926950747\n",
      "portugal 0.5277783714754396\n",
      "middlesex 0.5309681415244067\n",
      "new_zealand 0.5536123473155952\n",
      "south_africa 0.6229260206955374\n",
      "armenian 0.8730675481652141\n",
      "albanian 0.9247583985273815\n",
      "wrexham 0.9387674559805708\n",
      "genesis 0.9541958041958043\n",
      "parma 0.957065706570657\n",
      "darlington 0.9692185876181522\n",
      "glamorgan 0.9722208825657102\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model results. Higher values = easier separation of a word's two senses\n",
    "res1 = get_results('si_results', list(embeddings.keys()))\n",
    "\n",
    "sorted_res1 = sorted(res1.items(), key=lambda x: x[1])\n",
    "print(\"Model 1 Results:\")\n",
    "for item in sorted_res1:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfc6cbfc-e8f9-465c-a85c-4a672aaf211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Results:\n",
      "american 0.6532726671972702\n",
      "hiv 0.7045093890505157\n",
      "dutch 0.714530761364456\n",
      "scottish 0.7222987245800934\n",
      "portuguese 0.7287643034449356\n",
      "confederate 0.7296852401613763\n",
      "hindu 0.7298761430721148\n",
      "german 0.7650070215478468\n",
      "mexican 0.7661094748715378\n",
      "italian 0.7748689993238675\n",
      "oxford 0.8329721921271217\n",
      "union 0.833610400682012\n",
      "jewish 0.8526707612134747\n",
      "cambridge 0.8550757679388385\n",
      "chinese 0.8681893434059311\n",
      "japanese 0.8713872832369942\n",
      "persian 0.8831094478733208\n",
      "armenian 0.8836914430622665\n",
      "serbian 0.8902146045861197\n",
      "danish 0.9003957317504063\n",
      "wigan 0.9024086378737541\n",
      "russian 0.9074290955270213\n",
      "greek 0.9118544323728546\n",
      "norwegian 0.913595925358172\n",
      "swedish 0.9162692900645673\n",
      "hungarian 0.9186148985700033\n",
      "czech 0.9217703349282296\n",
      "united_states 0.9278182439853285\n",
      "finnish 0.9289295382101841\n",
      "middlesbrough 0.9374578708488743\n",
      "albanian 0.9387878787878787\n",
      "italy 0.9431395948193542\n",
      "new_zealand 0.9452389621859482\n",
      "wrexham 0.9458029197080291\n",
      "reading 0.9517601547388781\n",
      "south_africa 0.9527523438184526\n",
      "sunderland 0.9538608981380066\n",
      "parma 0.9548144163528779\n",
      "leeds 0.9559060017229827\n",
      "southampton 0.9573178398194777\n",
      "melbourne 0.958040863032055\n",
      "northern_ireland 0.9603611111111111\n",
      "republic_of_ireland 0.9609748416021342\n",
      "uruguay 0.9611508282476025\n",
      "dundee 0.9618623272633346\n",
      "glamorgan 0.9633333333333334\n",
      "genesis 0.9633366633366633\n",
      "liverpool 0.9636369898182819\n",
      "netherlands 0.9643305531167691\n",
      "germany 0.9644575629489609\n",
      "ireland 0.9648442092886538\n",
      "denmark 0.9650393461607829\n",
      "scotland 0.9655777946890973\n",
      "switzerland 0.9676439742928586\n",
      "cork 0.9677502138579983\n",
      "wales 0.9682114624505929\n",
      "sweden 0.970954954596936\n",
      "poland 0.9718812691373666\n",
      "canada 0.9733939522642008\n",
      "columbia 0.9739943292574872\n",
      "france 0.9747549194859864\n",
      "darlington 0.9749351983176016\n",
      "brazil 0.9750611810697647\n",
      "new_south_wales 0.9760364845620211\n",
      "great_britain 0.976491907049968\n",
      "barcelona 0.977258709217472\n",
      "milan 0.9777119329811034\n",
      "middlesex 0.9783183804922935\n",
      "java 0.9793168592026404\n",
      "english 0.9793496505892025\n",
      "portugal 0.979809822279089\n",
      "england 0.980293091065936\n",
      "mexico 0.9810354813998243\n",
      "warwickshire 0.9837624139243573\n",
      "leicestershire 0.9844273716717509\n",
      "somerset 0.9851258581235698\n",
      "hampshire 0.9854001646651336\n",
      "surrey 0.9870967741935484\n",
      "chelsea 0.9871055753262159\n",
      "derbyshire 0.9885258590615733\n",
      "northamptonshire 0.9885981268351229\n",
      "kent 0.9901117798796217\n",
      "nottinghamshire 0.9905365743527995\n",
      "celtic 0.9909406172069826\n",
      "essex 0.9914254223702059\n",
      "richmond 0.9926264090808156\n",
      "pc 0.9954954954954955\n"
     ]
    }
   ],
   "source": [
    "res2 = get_results('si_results_2', list(embeddings.keys()))\n",
    "\n",
    "sorted_res2 = sorted(res2.items(), key=lambda x: x[1])\n",
    "print(\"Model 2 Results:\")\n",
    "for item in sorted_res2:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e88aa9c-1dc5-41ab-8d79-79f02277987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - Model 1 Difference:\n",
      "glamorgan -0.008887549232376779\n",
      "parma -0.002251290217779145\n",
      "darlington 0.005716610699449398\n",
      "wrexham 0.007035463727458335\n",
      "genesis 0.009140859140859048\n",
      "armenian 0.0106238948970524\n",
      "albanian 0.014029480260497218\n",
      "american 0.15849835231279946\n",
      "hiv 0.18144825620042915\n",
      "hindu 0.21327333336585674\n",
      "dutch 0.2149021851328512\n",
      "scottish 0.21999997247409375\n",
      "portuguese 0.22392989041593514\n",
      "confederate 0.24085365726565233\n",
      "italian 0.25929087432386755\n",
      "mexican 0.26605722528953446\n",
      "german 0.2662154760267117\n",
      "south_africa 0.32982632312291527\n",
      "jewish 0.34190013014551357\n",
      "oxford 0.3420631012180308\n",
      "cambridge 0.3477563429739017\n",
      "chinese 0.3491431314501775\n",
      "union 0.36411665096004925\n",
      "persian 0.3812452872343396\n",
      "new_zealand 0.391626614870353\n",
      "japanese 0.3920077211931986\n",
      "greek 0.3954008357525719\n",
      "serbian 0.39890713332175187\n",
      "danish 0.39924630646305004\n",
      "russian 0.404181329989711\n",
      "swedish 0.41002638427569094\n",
      "norwegian 0.41118758257420307\n",
      "hungarian 0.41507534546868063\n",
      "wigan 0.4219066208410334\n",
      "czech 0.42229534447385764\n",
      "middlesbrough 0.4295563515868016\n",
      "finnish 0.4339925081783531\n",
      "melbourne 0.4407979552306366\n",
      "wales 0.4415955109243901\n",
      "netherlands 0.4438125821105643\n",
      "middlesex 0.4473502389678867\n",
      "germany 0.45069161698116655\n",
      "columbia 0.45196027907827574\n",
      "sweden 0.45198098942449183\n",
      "portugal 0.45203145080364937\n",
      "cork 0.45341222317270224\n",
      "republic_of_ireland 0.4558401614674541\n",
      "italy 0.4570328519229201\n",
      "milan 0.45863066089629767\n",
      "english 0.4591046782505033\n",
      "leicestershire 0.46222305864373847\n",
      "switzerland 0.4623704942318432\n",
      "sunderland 0.46544626399166505\n",
      "celtic 0.46594061720698254\n",
      "chelsea 0.46598956447139905\n",
      "leeds 0.4662395982488413\n",
      "liverpool 0.4679321704014368\n",
      "pc 0.4684479028004208\n",
      "somerset 0.47155996665070155\n",
      "java 0.47197965124520347\n",
      "mexico 0.47335487828950795\n",
      "richmond 0.47524485197707955\n",
      "barcelona 0.4769666291134751\n",
      "kent 0.4774811769082812\n",
      "northern_ireland 0.48130631579960353\n",
      "denmark 0.4818479172799556\n",
      "southampton 0.4831844907378029\n",
      "united_states 0.48584560204915467\n",
      "derbyshire 0.4866383312219297\n",
      "essex 0.48828958319699156\n",
      "poland 0.4893365356933526\n",
      "dundee 0.49134528122357773\n",
      "france 0.49622734669342927\n",
      "reading 0.4982682059838605\n",
      "brazil 0.49943342071893676\n",
      "surrey 0.5071285991699552\n",
      "northamptonshire 0.5189226704456706\n",
      "canada 0.5198431378368312\n",
      "warwickshire 0.5214761836427709\n",
      "uruguay 0.5225996226261491\n",
      "scotland 0.5242581625978934\n",
      "hampshire 0.5245167142102662\n",
      "new_south_wales 0.5277502717166165\n",
      "nottinghamshire 0.5332489295651548\n",
      "ireland 0.5361061797730247\n",
      "england 0.5552081389124115\n",
      "great_britain 0.557084343980164\n"
     ]
    }
   ],
   "source": [
    "dif = {}\n",
    "for word in list(embeddings.keys()):\n",
    "    dif[word] = res2[word] - res1[word]\n",
    "\n",
    "sorted_dif = sorted(dif.items(), key=lambda x: x[1])\n",
    "print(\"Model 2 - Model 1 Difference:\")\n",
    "for item in sorted_dif:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866d7ef-5f71-4387-a8a3-ad8e8b14f485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
